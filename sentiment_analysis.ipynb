{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a005a01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "40c490c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilities\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# plotting\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "\n",
    "# sklearn\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5a473d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, string\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "78a0eb6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "custom_tweet = 'Congrats #SportStar on your 7th best goal from last season winning goal of the year :) #Baller #Topbin #oneofmanyworldies'\n",
    "\n",
    "custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "print(classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b0489960",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/k8/zkdz2dhn7h1fw77td_v46cgr0000gn/T/ipykernel_98470/1463131910.py:1: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('clean_1.csv')\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('clean_1.csv')\n",
    "df['target'] = 0\n",
    "\n",
    "df.columns\n",
    "\n",
    "df=df.dropna(subset=['time_period_of_the_game', 'brand_ad_name', 'keywords', 'created_at', 'possibly_sensitive', 'id', 'lang', 'text', 'public_metrics.retweet_count', 'public_metrics.reply_count', 'public_metrics.like_count', 'public_metrics.quote_count', 'public_metrics.impression_count', 'name', 'username'])\n",
    "\n",
    "df=df[['created_at', 'id', 'text', 'username']]\n",
    "df['target'] = 0\n",
    "\n",
    "df = df.rename(columns={'created_at': 'date', 'id': 'ids', 'username': 'user'})\n",
    "\n",
    "# df.to_csv('clean_senti.csv', index=False)\n",
    "\n",
    "nltk.download('twitter_samples')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "print(pos_tag(tweet_tokens[0]))\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "\n",
    "from nltk import FreqDist\n",
    "\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "\n",
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "8667ebfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n",
      "Accuracy is: 0.9953333333333333\n",
      "Most Informative Features\n",
      "                      :( = True           Negati : Positi =   2069.4 : 1.0\n",
      "                      :) = True           Positi : Negati =   1003.2 : 1.0\n",
      "                follower = True           Positi : Negati =     38.2 : 1.0\n",
      "                     sad = True           Negati : Positi =     23.4 : 1.0\n",
      "                     bam = True           Positi : Negati =     20.9 : 1.0\n",
      "                     x15 = True           Negati : Positi =     17.7 : 1.0\n",
      "               community = True           Positi : Negati =     16.3 : 1.0\n",
      "                 welcome = True           Positi : Negati =     14.1 : 1.0\n",
      "                    poor = True           Negati : Positi =     13.7 : 1.0\n",
      "               goodnight = True           Positi : Negati =     12.3 : 1.0\n",
      "None\n",
      "I ordered just once from TerribleCo, they screwed up, never used the app again. Negative\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "\n",
    "import re, string, random\n",
    "\n",
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)\n",
    "\n",
    "    print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
    "\n",
    "    print(classifier.show_most_informative_features(10))\n",
    "\n",
    "    custom_tweet = \"I ordered just once from TerribleCo, they screwed up, never used the app again.\"\n",
    "\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "\n",
    "    print(custom_tweet, classifier.classify(dict([token, True] for token in custom_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9310133f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a7665a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    custom_tweet = row['text']\n",
    "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
    "    # compute the value for the new column for the current row\n",
    "    new_value = classifier.classify(dict([token, True] for token in custom_tokens))\n",
    "    # append the new value to the list\n",
    "    sentiment.append(new_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "302a30a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(Sentiment=sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7830b39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>ids</th>\n",
       "      <th>text</th>\n",
       "      <th>user</th>\n",
       "      <th>target</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-02-13T04:18:06.000Z</td>\n",
       "      <td>1.624986e+18</td>\n",
       "      <td>@DineshDSouza Reminds me of 7th grade 50years ...</td>\n",
       "      <td>HeyMon12924336</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-02-13T04:17:58.000Z</td>\n",
       "      <td>1.624986e+18</td>\n",
       "      <td>2023 MAYFEST planning has begun! Apply now to ...</td>\n",
       "      <td>mayfest</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-02-13T03:05:52.000Z</td>\n",
       "      <td>1.624968e+18</td>\n",
       "      <td>RT @Tamil1947: Progressive, conscientious &amp;amp...</td>\n",
       "      <td>sposhy0007</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-02-13T03:05:11.000Z</td>\n",
       "      <td>1.624968e+18</td>\n",
       "      <td>@joaomcd_ @DelusionPosting God bless u Brasili...</td>\n",
       "      <td>subhuman356255</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2023-02-13T03:01:05.000Z</td>\n",
       "      <td>1.624967e+18</td>\n",
       "      <td>50 ವರ್ಷಗಳ ಹಿಂದೆ: ಮಂಗಳವಾರ, 13–2–1973 \\n#50years...</td>\n",
       "      <td>prajavani</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2023-02-13T02:49:45.000Z</td>\n",
       "      <td>1.624964e+18</td>\n",
       "      <td>@50years_music 99 Luftballoons. Nena</td>\n",
       "      <td>freewill121</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2023-02-13T01:53:17.000Z</td>\n",
       "      <td>1.624950e+18</td>\n",
       "      <td>#Golden #jubilee #weddinganniversary #celebrat...</td>\n",
       "      <td>IamAdityaVerma</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2023-02-13T01:31:37.000Z</td>\n",
       "      <td>1.624944e+18</td>\n",
       "      <td>@ConflictedIn @SigmaAlbertan @moderndayenyo @a...</td>\n",
       "      <td>JosibeanL</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2023-02-13T01:27:19.000Z</td>\n",
       "      <td>1.624943e+18</td>\n",
       "      <td>RT @Tamil1947: Progressive, conscientious &amp;amp...</td>\n",
       "      <td>dennisckurian</td>\n",
       "      <td>0</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2023-02-13T01:23:00.000Z</td>\n",
       "      <td>1.624942e+18</td>\n",
       "      <td>Fifty Years Ago! #50years #BlueOysterCult  Tyr...</td>\n",
       "      <td>jefferygebhardt</td>\n",
       "      <td>0</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        date           ids  \\\n",
       "1   2023-02-13T04:18:06.000Z  1.624986e+18   \n",
       "2   2023-02-13T04:17:58.000Z  1.624986e+18   \n",
       "3   2023-02-13T03:05:52.000Z  1.624968e+18   \n",
       "4   2023-02-13T03:05:11.000Z  1.624968e+18   \n",
       "5   2023-02-13T03:01:05.000Z  1.624967e+18   \n",
       "6   2023-02-13T02:49:45.000Z  1.624964e+18   \n",
       "7   2023-02-13T01:53:17.000Z  1.624950e+18   \n",
       "8   2023-02-13T01:31:37.000Z  1.624944e+18   \n",
       "9   2023-02-13T01:27:19.000Z  1.624943e+18   \n",
       "10  2023-02-13T01:23:00.000Z  1.624942e+18   \n",
       "\n",
       "                                                 text             user  \\\n",
       "1   @DineshDSouza Reminds me of 7th grade 50years ...   HeyMon12924336   \n",
       "2   2023 MAYFEST planning has begun! Apply now to ...          mayfest   \n",
       "3   RT @Tamil1947: Progressive, conscientious &amp...       sposhy0007   \n",
       "4   @joaomcd_ @DelusionPosting God bless u Brasili...   subhuman356255   \n",
       "5   50 ವರ್ಷಗಳ ಹಿಂದೆ: ಮಂಗಳವಾರ, 13–2–1973 \\n#50years...        prajavani   \n",
       "6                @50years_music 99 Luftballoons. Nena      freewill121   \n",
       "7   #Golden #jubilee #weddinganniversary #celebrat...   IamAdityaVerma   \n",
       "8   @ConflictedIn @SigmaAlbertan @moderndayenyo @a...        JosibeanL   \n",
       "9   RT @Tamil1947: Progressive, conscientious &amp...    dennisckurian   \n",
       "10  Fifty Years Ago! #50years #BlueOysterCult  Tyr...  jefferygebhardt   \n",
       "\n",
       "    target Sentiment  \n",
       "1        0  Positive  \n",
       "2        0  Positive  \n",
       "3        0  Positive  \n",
       "4        0  Negative  \n",
       "5        0  Negative  \n",
       "6        0  Positive  \n",
       "7        0  Negative  \n",
       "8        0  Positive  \n",
       "9        0  Positive  \n",
       "10       0  Negative  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "57bc0aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('sentiment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a1ebaf01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"@joaomcd_ @DelusionPosting God bless u Brasilian bro but there's extremely minimal risk in real estate unless ur a braindead retard. my whole familys in it, they've been in it for 50years n they all say the same thing. don't be fooled by anyone saying otherwise as they aren't in it\\n\\np.s. I own a home\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee9670d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
